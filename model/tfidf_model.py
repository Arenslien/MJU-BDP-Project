# -*- coding: utf-8 -*-
"""TFIDF_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-D2GuncyFLrcyy7dQSwpTO7o-VhPROPG
"""

import pandas as pd
import numpy as np
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import euclidean_distances

"""⬇︎ TF-IDF Model"""

def tfidf_vec(doc_df, abstract):
  tfidfvectoriser = TfidfVectorizer()
  tfidfvectoriser.fit(doc_df)
  doc_vectors = tfidfvectoriser.transform(doc_df)
  abs_vectors = tfidfvectoriser.transform(abstract)

  return tfidfvectoriser, doc_vectors, abs_vectors

"""---



---

⬇︎ read csv
"""

cs_2021 = pd.read_csv('/content/drive/MyDrive/BDP/team/csv/paper_CS_2021.csv')

cs_2021.head()

cs_2021 = cs_2021.drop(['Unnamed: 0'], axis=1)
cs_2021.head()

cs_2021_copy = cs_2021.copy()

"""⬇︎ Data column type: int -> str"""

cs_2021_copy['Date'] = "% s" % 2022
print("Type after conversion : ", type(cs_2021_copy['Date'].loc[1]))

"""⬇︎ 각 column의 row를 하나로 합친 column 생성"""

cs_2021_copy['combined_text'] = cs_2021_copy['Title'] + '  ' + cs_2021_copy['Authors'] + '  ' + cs_2021_copy['Subjects'] + '  ' + cs_2021_copy['Abstract'] + '  ' + cs_2021_copy['Date']
cs_2021_copy['combined_text']

"""⬇︎ input data"""

# new abstract

new_abstract = 'Alzheimer’s disease (AD) is the most common late‐onset neurodegenerative disorder. \
Identifying individuals at increased risk of developing AD is important for early intervention. \
Using data from the Alzheimer Disease Genetics Consortium, we constructed polygenic risk scores (PRSs) for AD and age‐ at‐onset (AAO) of AD for the UK Biobank participants. \
We then built machine learning (ML) models for predicting development of AD, and explored feature importance among PRSs, conventional risk factors, \
and ICD‐10 codes from electronic health records, a total of > 11,000 features using the UK Biobank dataset. \
We used eXtreme Gradient Boosting (XGBoost) and SHapley Additive exPlanations (SHAP), which provided superior ML performance as well as aided ML model explanation. \
For participants age 40 and older, the area under the curve for AD was 0.88. For subjects of age 65 and older (late‐onset AD), PRSs were the most important predictors. \
This is the first observation that PRSs constructed from the AD risk and AAO play more important roles than age in predicting AD. \
The ML model also identified important predictors from EHR, including urinary tract infection, syncope and collapse, chest pain, disorientation and hypercholesterolemia, for developing AD. \
Our ML model improved the accuracy of AD risk prediction by efficiently exploring numerous predictors and identified novel feature patterns.'

"""
new_abstract = "NFTrig is a web-based application created for use as an educational tool to teach trigonometry and block chain technology. \
Creation of the application includes front and back end development as well as integration with other outside sources including MetaMask and OpenSea. \
The primary development languages include HTML, CSS (Bootstrap 5), and JavaScript as well as Solidity for smart contract creation. \
The application itself is hosted on Moralis utilizing their Web3 API. \
This technical report describes how the application was created, what the application requires, and smart contract design with security considerations in mind. \
The NFTrig application has underwent significant testing and validation prior to and after deployment. \
Future suggestions and recommendations for further development, maintenance, and use in other fields for education are also described."
"""

"""
new_abstract = "We present, to our knowledge, the first ap- plication of BERT to document classification. \
A few characteristics of the task might lead one to think that BERT is not the most appro- priate model: \
syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and doc- uments often have multiple labels. \
Neverthe- less, we show that a straightforward classifi- cation model using BERT is able to achieve the state of the art across four popular datasets. \
To address the computational expense associ- ated with BERT inference, we distill knowl- edge from BERTlarge to small bidirectional LSTMs, \
reaching BERTbase parity on multi-ple datasets using 30× fewer parameters. \
The primary contribution of our paper is improved baselines that can provide the foundation for future work."
"""

"""⬇︎ input data: 문장 -> '.'을 기준으로 나눈 후 list 생성"""

# '.'을 기준으로 나눈 후 리스트에 담기
abstract_list = [sentence.strip() for sentence in new_abstract.split('. ') if sentence]

# 결과 출력
print(abstract_list)

"""⬇︎ Model 실행"""

vectoriser_2021, doc_vectors_2021, abs_vectors_2021 = tfidf_vec(cs_2021_copy['combined_text'], abstract_list)

"""⬇︎ Cosine similarity"""

from sklearn.metrics.pairwise import cosine_similarity

sim_scores = cosine_similarity(doc_vectors_2021, abs_vectors_2021)
sim_scores